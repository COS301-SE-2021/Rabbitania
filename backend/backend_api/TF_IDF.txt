import sys
import nltk
import numpy as np
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from collections import Counter

### Weighting ###
alpha = 0.3

threadTitle = sys.argv[1].replace('[', '').replace(']', '').split(',')
threadBody = sys.argv[2].replace('[', '').replace(']', '').split(',')
newThreadTitle = sys.argv[3]
newThreadBody = sys.argv[4]

#####################
### PREPROCESSING ###
#####################

def convert_lower_case(data):
    return np.char.lower(data)

def remove_stop_words(data):
    stop_words = stopwords.words('english')
    words = word_tokenize(str(data))
    new_text = ""
    for w in words:
        if w not in stop_words and len(w) > 1:
            new_text = new_text + " " + w
    return new_text

def remove_punctuation(data):
    symbols = "!\"#$%&()*+-./:;<=>?@[\]^_`{|}~\n"
    for i in range(len(symbols)):
        data = np.char.replace(data, symbols[i], ' ')
        data = np.char.replace(data, "  ", " ")
    return data

def remove_apostrophe(data):
    return np.char.replace(data, "'", "")

def stemming(data):
    stemmer = PorterStemmer()
    tokens = word_tokenize(str(data))
    new_text = ""
    for w in tokens:
        new_text = new_text + " " + stemmer.stem(w)
    return new_text

def preprocess(data):
    data = convert_lower_case(data)
    data = remove_punctuation(data)
    data = remove_stop_words(data)
    data = remove_apostrophe(data)
    data = stemming(data)
    return data

processed_texts = []
processed_titles = []

Ntitles = len(threadTitle)
Nbodies = len(threadBody)

for i in range(0, len(threadTitle)):
    TitlesText = threadTitle[i]
    processed_titles.append(word_tokenize(str(preprocess(TitlesText))))

for i in range(0, len(threadBody)):
    BodyText = threadBody[i]
    processed_texts.append(word_tokenize(str(preprocess(BodyText))))

# print(processed_texts)
# print(processed_titles)

###################################
### Calculating DF for all words ###
###################################

DF = {}

for i in range(Ntitles):
    tokens = processed_titles[i]
    for w in tokens:
        try:
            DF[w].add(i)
        except:
            DF[w] = {i}

for i in range(Nbodies):
    tokens = processed_texts[i]
    for w in tokens:
        try:
            DF[w].add(i)
        except:
            DF[w] = {i}
for i in DF:
    DF[i] = len(DF[i])

# print(DF)


def doc_freq(word):
    c = 0
    try:
        c = DF[word]
    except:
        pass
    return c


###########################################################################
### Calculating TF-IDF for body, and will add the title weights to this ###
###########################################################################

doc = 0

tf_idf = {}

for i in range(Nbodies):
    tokens = processed_texts[i]
    counter = Counter(tokens + processed_titles[i])
    words_count = len(tokens + processed_titles[i])

    for token in np.unique(tokens):
        tf = counter[token]/words_count
        df = doc_freq(token)
        idf = np.log((Ntitles + 1)/ (df + 1))
        tf_idf[doc, token] = tf*idf
    doc += 1
# print(tf_idf)


########################################################################
### Calculate tf_idf for the title, will be added to the body tf_idf ###
########################################################################

doc = 0

tf_idf_title = {}
for i in range(Ntitles):
    tokens = processed_titles[i]
    counter = Counter(tokens + processed_texts[i])
    words_count = len(tokens + processed_texts[i])

    for token in np.unique(tokens):
        tf = counter[token]/words_count
        df = doc_freq(token)
        idf = np.log((Nbodies+1)/(df + 1))

        tf_idf_title[doc, token] = tf*idf
    doc += 1

print(tf_idf_title)

###############################################
### Merging the TF-IDF according to weights ###
###############################################

for i in tf_idf:
    tf_idf[i] *= alpha

for i in tf_idf_title:
    tf_idf[i] = tf_idf_title[i]

# print(len(tf_idf))

def matching_score(k, query):
    preprocessed_query = preprocess(query)
    tokens = word_tokenize(str(preprocessed_query))

    print("Matching Score")
    print("\nQuery:", query)
    print("")
    print(tokens)
    
    query_weights = {}

    for key in tf_idf:
        
        if key[1] in tokens:
            try:
                query_weights[key[0]] += tf_idf[key]
            except:
                query_weights[key[0]] = tf_idf[key]
    
    query_weights = sorted(query_weights.items(), key=lambda x: x[1], reverse=True)
    print(query_weights)

    if(query_weights != []):
        if query_weights[0][1] > 0.065:
            print("A thread like this possible already exists")
        else:
            print("Thread can be created")
    else:
        print("Thread can be created")

    
    l = []
    
    for i in query_weights[:k]:
        l.append(i[0])
    
    print(l)
    

matching_score(1, "Stupid Questions Two")


# ["Stupid Questions","Second thread title is about cats and other memerobelia","Third thread title is just because I need one"] 

# ["Stupid Question #1","I really have no idea what I'm doing","Hopefully I can get this done tonight but I don't know for sure"] 

# "this is the thread title that we are testing against the titles and bodies of the threads in the database" 

# "This is a the query body to match against the other forum threads"
